{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N3cO6EscqAdv",
        "8Ennl1NoqEAc",
        "x008szdYqPmr",
        "ji7iC5l9qc28",
        "1_R1H1qiqy9n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries\n",
        "First, the necessary libraries are installed and imported. `pygame` is installed to create the game environment, and the FlappyBird assets are cloned from a GitHub repository.\n"
      ],
      "metadata": {
        "id": "N3cO6EscqAdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame\n",
        "!git clone https://github.com/yanpanlau/Keras-FlappyBird.git"
      ],
      "metadata": {
        "id": "XZN_6BxFqCpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the Environment\n",
        "\n",
        "The `pygame` library is used to create the FlappyBird game environment. The game assets are loaded, and the FlappyBird class defines the game's mechanics and state updates."
      ],
      "metadata": {
        "id": "8Ennl1NoqEAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "from itertools import cycle\n",
        "from random import randint\n",
        "from pygame.image import load\n",
        "from pygame.surfarray import pixels_alpha\n",
        "from pygame.transform import rotate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eETIvbiiqF8x",
        "outputId": "55fe593b-b9c8-4daf-aca2-cb85d46ff489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.6.0 (SDL 2.28.4, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(sprites_path):\n",
        "    base_image = load(sprites_path + 'base.png').convert_alpha()\n",
        "    background_image = load(sprites_path + 'background-black.png').convert()\n",
        "    pipe_images = [rotate(load(sprites_path + 'pipe-green.png').convert_alpha(), 180), load(sprites_path + 'pipe-green.png').convert_alpha()]\n",
        "    bird_images = [load(sprites_path + 'redbird-upflap.png').convert_alpha(), load(sprites_path + 'redbird-midflap.png').convert_alpha(), load(sprites_path + 'redbird-downflap.png').convert_alpha()]\n",
        "    bird_hitmask = [pixels_alpha(image).astype(bool) for image in bird_images]\n",
        "    pipe_hitmask = [pixels_alpha(image).astype(bool) for image in pipe_images]\n",
        "\n",
        "    return base_image, background_image, pipe_images, bird_images, bird_hitmask, pipe_hitmask"
      ],
      "metadata": {
        "id": "z7eo6cHsqHhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pygame.init()\n",
        "fps_clock = pygame.time.Clock()\n",
        "fps = 30\n",
        "\n",
        "screen_width = 288\n",
        "screen_height = 512\n",
        "screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "pygame.display.set_caption('Flappy Bird')\n",
        "\n",
        "base_image, background_image, pipe_images, bird_images, bird_hitmask, pipe_hitmask = load_images('/content/Keras-FlappyBird/assets/sprites/')\n",
        "\n",
        "bird_width = bird_images[0].get_width()\n",
        "bird_height = bird_images[0].get_height()\n",
        "pipe_width = pipe_images[0].get_width()\n",
        "pipe_height = pipe_images[0].get_height()\n",
        "pipe_gap_size = 100\n",
        "\n",
        "bird_index_gen = cycle([0, 1, 2, 1])"
      ],
      "metadata": {
        "id": "W9HV1fJXqJsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `FlappyBird` class handles the game's logic, including initializing the game state, generating pipes, checking for collisions, and updating the game state based on actions taken by the agent."
      ],
      "metadata": {
        "id": "z5h2iJCQqKdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlappyBird(object):\n",
        "    def __init__(self):\n",
        "        self.pipe_vel_x = -4\n",
        "        self.min_velocity_y = -8\n",
        "        self.max_velocity_y = 10\n",
        "        self.downward_speed = 1\n",
        "        self.upward_speed = -9\n",
        "        self.cur_velocity_y = 0\n",
        "        self.iter = self.bird_index = self.score = 0\n",
        "        self.bird_x = int(screen_width / 5)\n",
        "        self.bird_y = int((screen_height - bird_height) / 2)\n",
        "        self.base_x = 0\n",
        "        self.base_y = screen_height * 0.79\n",
        "        self.base_shift = base_image.get_width() - background_image.get_width()\n",
        "        self.pipes = [self.gen_random_pipe(screen_width), self.gen_random_pipe(screen_width * 1.5)]\n",
        "        self.is_flapped = False\n",
        "\n",
        "    def gen_random_pipe(self, x):\n",
        "        gap_y = randint(2, 10) * 10 + int(self.base_y * 0.2)\n",
        "        return {\"x_upper\": x,\n",
        "                \"y_upper\": gap_y - pipe_height,\n",
        "                \"x_lower\": x,\n",
        "                \"y_lower\": gap_y + pipe_gap_size}\n",
        "\n",
        "    def check_collision(self):\n",
        "        if bird_height + self.bird_y >= self.base_y - 1:\n",
        "            return True\n",
        "        bird_rect = pygame.Rect(self.bird_x, self.bird_y, bird_width, bird_height)\n",
        "        for pipe in self.pipes:\n",
        "            pipe_boxes = [pygame.Rect(pipe[\"x_upper\"], pipe[\"y_upper\"], pipe_width, pipe_height), pygame.Rect(pipe[\"x_lower\"], pipe[\"y_lower\"], pipe_width, pipe_height)]\n",
        "            if bird_rect.collidelist(pipe_boxes) == -1:\n",
        "                return False\n",
        "            for i in range(2):\n",
        "                cropped_bbox = bird_rect.clip(pipe_boxes[i])\n",
        "                x1 = cropped_bbox.x - bird_rect.x\n",
        "                y1 = cropped_bbox.y - bird_rect.y\n",
        "                x2 = cropped_bbox.x - pipe_boxes[i].x\n",
        "                y2 = cropped_bbox.y - pipe_boxes[i].y\n",
        "                for x in range(cropped_bbox.width):\n",
        "                    for y in range(cropped_bbox.height):\n",
        "                        if bird_hitmask[self.bird_index][x1+x, y1+y] and pipe_hitmask[i][x2+x, y2+y]:\n",
        "                            return True\n",
        "        return False\n",
        "\n",
        "    def next_step(self, action):\n",
        "        pygame.event.pump()\n",
        "        reward = 0.1\n",
        "        if action == 1:\n",
        "            self.cur_velocity_y = self.upward_speed\n",
        "            self.is_flapped = True\n",
        "\n",
        "        bird_center_x = self.bird_x + bird_width / 2\n",
        "        for pipe in self.pipes:\n",
        "            pipe_center_x = pipe[\"x_upper\"] + pipe_width / 2\n",
        "            if pipe_center_x < bird_center_x < pipe_center_x + 5:\n",
        "                self.score += 1\n",
        "                reward = 1\n",
        "                break\n",
        "\n",
        "        if (self.iter + 1) % 3 == 0:\n",
        "            self.bird_index = next(bird_index_gen)\n",
        "        self.iter = (self.iter + 1) % fps\n",
        "        self.base_x = -((-self.base_x + 100) % self.base_shift)\n",
        "\n",
        "        if self.cur_velocity_y < self.max_velocity_y and not self.is_flapped:\n",
        "            self.cur_velocity_y += self.downward_speed\n",
        "            self.is_flapped = False\n",
        "            self.bird_y += min(self.cur_velocity_y, self.bird_y - self.cur_velocity_y - bird_height)\n",
        "        if self.bird_y < 0:\n",
        "            self.bird_y = 0\n",
        "\n",
        "        for pipe in self.pipes:\n",
        "            pipe[\"x_upper\"] += self.pipe_vel_x\n",
        "            pipe[\"x_lower\"] += self.pipe_vel_x\n",
        "\n",
        "        if 0 < self.pipes[0][\"x_lower\"] < 5:\n",
        "            self.pipes.append(self.gen_random_pipe(screen_width + 10))\n",
        "\n",
        "        if self.pipes[0][\"x_lower\"] < -pipe_width:\n",
        "            self.pipes.pop(0)\n",
        "\n",
        "        if self.check_collision():\n",
        "            is_done = True\n",
        "            reward = -1\n",
        "            self.__init__()\n",
        "        else:\n",
        "            is_done = False\n",
        "\n",
        "        screen.blit(background_image, (0, 0))\n",
        "        screen.blit(base_image, (self.base_x, self.base_y))\n",
        "        screen.blit(bird_images[self.bird_index], (self.bird_x, self.bird_y))\n",
        "        for pipe in self.pipes:\n",
        "            screen.blit(pipe_images[0], (pipe[\"x_upper\"], pipe[\"y_upper\"]))\n",
        "            screen.blit(pipe_images[1], (pipe[\"x_lower\"], pipe[\"y_lower\"]))\n",
        "        image = pygame.surfarray.array3d(pygame.display.get_surface())\n",
        "        pygame.display.update()\n",
        "        fps_clock.tick(fps)\n",
        "        return image, reward, is_done"
      ],
      "metadata": {
        "id": "xW1pdI6nqOz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the DQN Model\n",
        "\n",
        "The DQN model is implemented using PyTorch. The model architecture includes convolutional layers followed by fully connected layers to process the game's image frames.\n"
      ],
      "metadata": {
        "id": "x008szdYqPmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "UOZSdghyqSmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNModel(nn.Module):\n",
        "    def __init__(self, n_action=2):\n",
        "        super(DQNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.out = nn.Linear(512, n_action)\n",
        "        self._create_weights()\n",
        "\n",
        "    def _create_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc(x))\n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "I84LcVCXqUiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, n_action, lr=1e-6):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = DQNModel(n_action)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "    def predict(self, s):\n",
        "        return self.model(torch.Tensor(s))\n",
        "\n",
        "    def update(self, y_predict, y_target):\n",
        "        loss = self.criterion(y_predict, y_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*replay_data)\n",
        "            state_batch = torch.cat(tuple(state for state in state_batch))\n",
        "            next_state_batch = torch.cat(tuple(state for state in next_state_batch))\n",
        "            q_values_batch = self.predict(state_batch)\n",
        "            q_values_next_batch = self.predict(next_state_batch)\n",
        "            reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n",
        "            action_batch = torch.from_numpy(np.array([[1, 0] if action == 0 else [0, 1] for action in action_batch], dtype=np.float32))\n",
        "            q_value = torch.sum(q_values_batch * action_batch, dim=1)\n",
        "            td_targets = torch.cat(tuple(reward if terminal else reward + gamma * torch.max(prediction) for reward, terminal, prediction in zip(reward_batch, done_batch, q_values_next_batch)))\n",
        "            loss = self.update(q_value, td_targets)\n",
        "            return loss"
      ],
      "metadata": {
        "id": "N2tJaDAuqaHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Fine-Tuning the Network\n",
        "\n",
        "The DQN model is trained using experience replay and an epsilon-greedy policy for action selection. The training loop pre-processes the game frames and updates the model weights based on the replay buffer."
      ],
      "metadata": {
        "id": "ji7iC5l9qc28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "ZAVBcR7iqecj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "GRlrqrj7qgEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 84\n",
        "batch_size = 32\n",
        "lr = 1e-6\n",
        "gamma = 0.99\n",
        "init_epsilon = 0.1\n",
        "final_epsilon = 1e-4\n",
        "n_iter = 2000000\n",
        "memory_size = 50000\n",
        "n_action = 2\n",
        "\n",
        "!mkdir trained_models\n"
      ],
      "metadata": {
        "id": "3gVaBADtqiO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_path = 'trained_models'"
      ],
      "metadata": {
        "id": "lQswdU6ZqlAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "estimator = DQN(n_action)\n",
        "\n",
        "memory = deque(maxlen=memory_size)\n",
        "\n",
        "env = FlappyBird()\n",
        "\n",
        "image, reward, is_done = env.next_step(0)"
      ],
      "metadata": {
        "id": "u0s1yHeWqjp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def pre_processing(image, width, height):\n",
        "    image = cv2.cvtColor(cv2.resize(image, (width, height)), cv2.COLOR_BGR2GRAY)\n",
        "    _, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\n",
        "    return image[None, :, :].astype(np.float32)"
      ],
      "metadata": {
        "id": "RVVJcCYYqpLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = pre_processing(image[:screen_width, :int(env.base_y)], image_size, image_size)\n",
        "\n",
        "image = torch.from_numpy(image)\n",
        "state = torch.cat(tuple(image for _ in range(4)))[None, :, :, :]"
      ],
      "metadata": {
        "id": "LyDV1-qVqulh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(n_iter):\n",
        "    epsilon = final_epsilon + (n_iter - iter) * (init_epsilon - final_epsilon) / n_iter\n",
        "    policy = gen_epsilon_greedy_policy(estimator, epsilon, n_action)\n",
        "    action = policy(state)\n",
        "    next_image, reward, is_done = env.next_step(action)\n",
        "    next_image = pre_processing(next_image[:screen_width, :int(env.base_y)], image_size, image_size)\n",
        "    next_image = torch.from_numpy(next_image)\n",
        "    next_state = torch.cat((state[0, 1:, :, :], next_image))[None, :, :, :]\n",
        "    memory.append([state, action, next_state, reward, is_done])\n",
        "    loss = estimator.replay(memory, batch_size, gamma)\n",
        "    state = next_state\n",
        "    print(\"Iteration: {}/{}, Action: {}, Loss: {}, Epsilon: {}, Reward: {}\".format(iter + 1, n_iter, action, loss, epsilon, reward))\n",
        "    if iter + 1 % 10000 == 0:\n",
        "        torch.save(estimator.model, \"{}/{}\".format(saved_path, iter + 1))\n",
        "\n",
        "torch.save(estimator.model, \"{}/final\".format(saved_path))"
      ],
      "metadata": {
        "id": "ESrzCxajqxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improvements"
      ],
      "metadata": {
        "id": "1_R1H1qiqy9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Code Organization and Modularity**:\n",
        "   - Split the code into multiple files or modules to improve readability and maintainability.\n",
        "   - Use a separate configuration file for hyperparameters.\n",
        "\n",
        "2. **Comments and Documentation**:\n",
        "   - Add more comments and docstrings to explain each part of the code.\n",
        "   - Provide detailed documentation on how to use and modify the code.\n",
        "\n",
        "3. **Error Handling**:\n",
        "   - Add error handling to manage edge cases and potential errors during training.\n",
        "\n",
        "4. **Logging and Monitoring**:\n",
        "   - Use a logging library to log the training progress, hyperparameters, and performance metrics.\n",
        "   - Implement a monitoring system to visualize training progress (e.g., TensorBoard).\n",
        "\n",
        "5. **Model Saving and Loading**:\n",
        "   - Implement functions to save and load models, including the replay buffer and optimizer state.\n",
        "   - Periodically save models during training to prevent data loss.\n",
        "\n",
        "6. **Performance Optimization**:\n",
        "   - Optimize the data processing pipeline to handle larger batches more efficiently.\n",
        "   - Use GPU acceleration for model training.\n",
        "\n",
        "7. **Exploration Strategies**:\n",
        "   - Experiment with different exploration strategies (e.g., decaying epsilon, noisy networks).\n",
        "\n",
        "8. **Advanced Techniques**:\n",
        "   - Implement Double DQN to reduce overestimation bias.\n",
        "   - Use prioritized experience replay to sample more important transitions.\n",
        "\n",
        "By implementing these improvements, the code will become more robust, maintainable, and efficient, allowing for better training and performance of the DQN model."
      ],
      "metadata": {
        "id": "jcnz_tQpq0_K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb-fnhZoq1Nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}