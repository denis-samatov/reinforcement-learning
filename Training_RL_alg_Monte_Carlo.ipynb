{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XE3S7XOR_DSw",
        "4WIL_zhDClr6",
        "wyNy4V1bQdvq",
        "4ukopKSTY6fL",
        "lLAE3OiZjj9r",
        "_ftJLR5KgdGw",
        "55VEIu14jqTw",
        "RbeefynykwDQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Blackjack with Monte Carlo Method"
      ],
      "metadata": {
        "id": "XE3S7XOR_DSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "Blackjack is a popular card game where the objective is to accumulate a hand value as close to 21 as possible without exceeding it. The game involves probabilistic decisions, making it a suitable candidate for reinforcement learning techniques like Monte Carlo methods.\n",
        "\n",
        "**Monte Carlo methods** involve using randomness to solve problems that might be deterministic in principle. In the context of Blackjack, Monte Carlo methods are used to evaluate and improve the strategies by simulating a large number of games. The key idea is to average the returns (rewards) following visits to a particular state to estimate the value function, which is the expected return starting from that state.\n",
        "\n",
        "**Steps:**\n",
        "1. **Simulate Episodes:** Play the game repeatedly by following a certain policy.\n",
        "2. **Track Returns:** For each state encountered during the episodes, track the rewards received.\n",
        "3. **Update Value Function:** Average the returns to update the value function for each state.\n"
      ],
      "metadata": {
        "id": "ak_oareXjJEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "t_2ZtMtgccE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "fDkCSyDdcc_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, hold_score):\n",
        "    \"\"\"\n",
        "    Run a single episode of Blackjack until the game is done.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    hold_score (int): The score at which the player will hold (stop taking cards).\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        # Take action: hit (1) if score < hold_score, else stick (0)\n",
        "        action = 1 if state[0] < hold_score else 0\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return states, rewards"
      ],
      "metadata": {
        "id": "UtMZC4cEcfJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_prediction_first_visit(env, hold_score, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Monte Carlo prediction algorithm to estimate the value function.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    hold_score (int): The score at which the player will hold.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    V = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        states_t, rewards_t = run_episode(env, hold_score)\n",
        "        return_t = 0\n",
        "        G = {}\n",
        "\n",
        "        for state_t, reward_t in zip(states_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            if state_t not in G:\n",
        "                G[state_t] = return_t\n",
        "\n",
        "        for state, return_t in G.items():\n",
        "            if state[0] <= 21:  # Consider only valid states\n",
        "                V[state] += return_t\n",
        "                N[state] += 1\n",
        "\n",
        "    for state in V:\n",
        "        V[state] /= N[state]\n",
        "\n",
        "    return V"
      ],
      "metadata": {
        "id": "aHcypgMWch_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OHa8YWXecjy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "g47Cowe8co0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "hold_score = 18\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "\n",
        "# Run Monte Carlo prediction\n",
        "value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)\n",
        "\n",
        "# Print the value function\n",
        "print('Value function estimated using First-Visit MC:\\n', value)\n",
        "print('Number of states:', len(value))\n",
        "\n",
        "# Plot the value function\n",
        "plot_blackjack_value(value)"
      ],
      "metadata": {
        "id": "fm0sOSa0csNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Monte Carlo Control with On-Policy Strategy"
      ],
      "metadata": {
        "id": "4WIL_zhDClr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "**On-policy Monte Carlo control** aims to find the optimal policy while using and improving the policy being followed. The policy is improved iteratively based on the value function derived from the returns of the simulated episodes.\n",
        "\n",
        "**Steps:**\n",
        "1. **Initialize Policy and Q-Function:** Start with an initial policy and Q-function.\n",
        "2. **Simulate Episodes:** Follow the current policy to generate episodes.\n",
        "3. **Update Q-Function:** Use the returns from the episodes to update the Q-function.\n",
        "4. **Improve Policy:** Use the updated Q-function to improve the policy by making it greedy with respect to the Q-function."
      ],
      "metadata": {
        "id": "DVWIz6ffjUQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "mBNkuxdLdIXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "8bbPoBaAdJUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, Q, n_action):\n",
        "    \"\"\"\n",
        "    Executes an episode following the given Q-function.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    Q (defaultdict): The Q-function.\n",
        "    n_action (int): The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    actions (list): List of actions taken during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    is_done = False\n",
        "\n",
        "    # Start with a random action\n",
        "    action = torch.randint(0, n_action, [1]).item()\n",
        "\n",
        "    while not is_done:\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "        # Choose the next action based on the current Q-function\n",
        "        action = torch.argmax(Q[state]).item()\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "xCR_QUBOdLbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_on_policy(env, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Finds the optimal policy using on-policy Monte Carlo control.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "    Q (defaultdict): The optimal Q-function.\n",
        "    policy (dict): The optimal policy.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        states_t, actions_t, rewards_t = run_episode(env, Q, n_action)\n",
        "        return_t = 0\n",
        "        G = {}\n",
        "\n",
        "        # Calculate the return for each state-action pair\n",
        "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            if (state_t, action_t) not in G:\n",
        "                G[(state_t, action_t)] = return_t\n",
        "\n",
        "        # Update Q-function based on the returns\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:  # Consider only valid states\n",
        "                G_sum[state_action] += return_t\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    # Derive the optimal policy from the Q-function\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "3D8xzU1WdM1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7bAvz23EdOvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "Lg0kGZgkdQvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "\n",
        "# Run Monte Carlo control to find the optimal policy and Q-function\n",
        "optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\\n\", optimal_policy)\n",
        "\n",
        "# Derive the value function from the optimal Q-function\n",
        "optimal_value = defaultdict(float)\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\\n\", optimal_value)\n",
        "\n",
        "# Plot the optimal value function\n",
        "plot_blackjack_value(optimal_value)"
      ],
      "metadata": {
        "id": "DWj9nSlgCf5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Monte Carlo Method with ε-Greedy Strategy"
      ],
      "metadata": {
        "id": "wyNy4V1bQdvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "The **ε-greedy strategy** is used to balance exploration and exploitation in reinforcement learning. It ensures that the agent occasionally explores new actions to discover their potential benefits while mostly exploiting known actions that yield the highest rewards.\n",
        "\n",
        "**Steps:**\n",
        "1. **Initialize Q-Function:** Start with an initial Q-function.\n",
        "2. **Simulate Episodes:** Use the ε-greedy policy to generate episodes, where the agent chooses a random action with probability ε and the best-known action with probability 1-ε.\n",
        "3. **Update Q-Function:** Use the returns from the episodes to update the Q-function.\n",
        "4. **Improve Policy:** The policy becomes greedy with respect to the updated Q-function."
      ],
      "metadata": {
        "id": "mP21ES1ajcuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "LillwQzMd7ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "s8BLGls_d81a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, Q, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Executes an episode following the ε-greedy strategy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    Q (defaultdict): The Q-function.\n",
        "    epsilon (float): The exploration-exploitation trade-off parameter.\n",
        "    n_action (int): The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    actions (list): List of actions taken during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        # Epsilon-greedy action selection\n",
        "        probs = torch.ones(n_action) * epsilon / n_action\n",
        "        best_action = torch.argmax(Q[state]).item()\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "EJ5P0qJhd_gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):\n",
        "    \"\"\"\n",
        "    Finds the optimal ε-greedy policy using Monte Carlo control.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "    epsilon (float): The exploration-exploitation trade-off parameter.\n",
        "\n",
        "    Returns:\n",
        "    Q (defaultdict): The optimal Q-function.\n",
        "    policy (dict): The optimal policy.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)\n",
        "        return_t = 0\n",
        "        G = {}\n",
        "\n",
        "        # Calculate the return for each state-action pair\n",
        "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            if (state_t, action_t) not in G:\n",
        "                G[(state_t, action_t)] = return_t\n",
        "\n",
        "        # Update Q-function based on the returns\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:  # Consider only valid states\n",
        "                G_sum[state_action] += return_t\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    # Derive the optimal policy from the Q-function\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "2GBU-AlJeCSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Simulates an episode using a given policy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    policy (dict): The policy to follow.\n",
        "\n",
        "    Returns:\n",
        "    reward (int): The reward received at the end of the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        action = policy[state]\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "3mzuJADEeEuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2THGb5pCeHmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "9IbQk8z9eMBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "epsilon = 0.1\n",
        "\n",
        "# Run Monte Carlo control to find the optimal ε-greedy policy and Q-function\n",
        "optimal_Q, optimal_policy = mc_control_epsilon_greedy(env, gamma, n_episode, epsilon)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\\n\", optimal_policy)\n",
        "\n",
        "# Derive the value function from the optimal Q-function\n",
        "optimal_value = defaultdict(float)\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\\n\", optimal_value)\n",
        "\n",
        "# Plot the optimal value function\n",
        "plot_blackjack_value(optimal_value)"
      ],
      "metadata": {
        "id": "Pd7wkX1aPK8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate episodes to evaluate the optimal policy\n",
        "n_episode = 100000\n",
        "n_win_optimal = 0\n",
        "n_lose_optimal = 0\n",
        "\n",
        "for _ in range(n_episode):\n",
        "    reward = simulate_episode(env, optimal_policy)\n",
        "    if reward == 1:\n",
        "        n_win_optimal += 1\n",
        "    elif reward == -1:\n",
        "        n_lose_optimal += 1\n",
        "\n",
        "# Print the win/lose probabilities\n",
        "print('Win probability with optimal policy: {:.2f}'.format(n_win_optimal / n_episode))\n",
        "print('Lose probability with optimal policy: {:.2f}'.format(n_lose_optimal / n_episode))"
      ],
      "metadata": {
        "id": "x5uYXg-eePeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Monte Carlo Control with Off-Policy Strategy"
      ],
      "metadata": {
        "id": "4ukopKSTY6fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "**Off-policy Monte Carlo control** involves learning an optimal policy while following a different behavior policy. This approach uses importance sampling to correct the difference between the behavior policy and the target policy.\n",
        "\n",
        "**Steps:**\n",
        "1. **Behavior Policy:** Define a behavior policy that is used to generate episodes.\n",
        "2. **Simulate Episodes:** Follow the behavior policy to generate episodes.\n",
        "3. **Importance Sampling:** Use importance sampling to weight the returns, correcting for the difference between the behavior policy and the target policy.\n",
        "4. **Update Q-Function:** Use the weighted returns to update the Q-function.\n",
        "5. **Improve Policy:** The policy is improved based on the updated Q-function."
      ],
      "metadata": {
        "id": "lLAE3OiZjj9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "2vDA6CiNgaBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "NEesR7CxgbVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_random_policy(n_action):\n",
        "    \"\"\"\n",
        "    Generates a random policy with equal probability for each action.\n",
        "\n",
        "    Parameters:\n",
        "    n_action (int): Number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "    function: A policy function that returns action probabilities for a given state.\n",
        "    \"\"\"\n",
        "    probs = torch.ones(n_action) / n_action\n",
        "    def policy_function(state):\n",
        "        return probs\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "hlS475Mdgc0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = gen_random_policy(env.action_space.n)"
      ],
      "metadata": {
        "id": "xf3PGS2Pgdu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, behavior_policy):\n",
        "    \"\"\"\n",
        "    Executes an episode following the given behavior policy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    actions (list): List of actions taken during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        probs = behavior_policy(state)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "a0ON97q5ggRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_off_policy(env, gamma, n_episode, behavior_policy):\n",
        "    \"\"\"\n",
        "    Finds the optimal policy using off-policy Monte Carlo control with importance sampling.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    Q (defaultdict): The optimal Q-function.\n",
        "    policy (dict): The optimal policy.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)\n",
        "        return_t = 0\n",
        "        G = {}\n",
        "        w = 1  # Importance sampling ratio\n",
        "\n",
        "        # Calculate the return for each state-action pair\n",
        "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            G[(state_t, action_t)] = return_t\n",
        "\n",
        "            if action_t != torch.argmax(Q[state_t]).item():\n",
        "                break\n",
        "            w *= 1.0 / behavior_policy(state_t)[action_t]\n",
        "\n",
        "        # Update Q-function based on the returns\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:  # Consider only valid states\n",
        "                G_sum[state_action] += return_t * w\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    # Derive the optimal policy from the Q-function\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "Cb5g9W8OgjSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "y4Zi45tbglJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "bkj9kPGmgnvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "\n",
        "# Run Monte Carlo control to find the optimal policy and Q-function\n",
        "optimal_Q, optimal_policy = mc_control_off_policy(env, gamma, n_episode, random_policy)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\\n\", optimal_policy)\n",
        "\n",
        "# Derive the value function from the optimal Q-function\n",
        "optimal_value = defaultdict(float)\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\\n\", optimal_value)\n",
        "\n",
        "# Plot the optimal value function\n",
        "plot_blackjack_value(optimal_value)"
      ],
      "metadata": {
        "id": "TgV6I1dhfmgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Incremental Implementation of the Previous Method\n"
      ],
      "metadata": {
        "id": "_ftJLR5KgdGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "**Incremental Monte Carlo control** updates the Q-function incrementally after each episode, rather than waiting for a batch of episodes. This approach is computationally efficient and helps in real-time learning scenarios.\n",
        "\n",
        "**Steps:**\n",
        "1. **Initialize Q-Function and Count:** Start with initial Q-values and counts for state-action pairs.\n",
        "2. **Simulate Episodes:** Use a behavior policy to generate episodes.\n",
        "3. **Update Incrementally:** For each state-action pair in the episode, update the Q-function incrementally using the observed return and importance sampling weights.\n",
        "4. **Improve Policy:** Update the policy to be greedy with respect to the incrementally updated Q-function."
      ],
      "metadata": {
        "id": "55VEIu14jqTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "-W3Jijo-hHuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "8iTc7yjmhJSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_random_policy(n_action):\n",
        "    \"\"\"\n",
        "    Generates a random policy with equal probability for each action.\n",
        "\n",
        "    Parameters:\n",
        "    n_action (int): Number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "    function: A policy function that returns action probabilities for a given state.\n",
        "    \"\"\"\n",
        "    probs = torch.ones(n_action) / n_action\n",
        "    def policy_function(state):\n",
        "        return probs\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "2YA-KCAChLyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = gen_random_policy(env.action_space.n)"
      ],
      "metadata": {
        "id": "ZxTMuS4RhNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, behavior_policy):\n",
        "    \"\"\"\n",
        "    Executes an episode following the given behavior policy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    actions (list): List of actions taken during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        probs = behavior_policy(state)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "vsx1iKPohQAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_off_policy_incremental(env, gamma, n_episode, behavior_policy):\n",
        "    \"\"\"\n",
        "    Finds the optimal policy using off-policy Monte Carlo control with incremental updates.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    Q (defaultdict): The optimal Q-function.\n",
        "    policy (dict): The optimal policy.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        W = 1.0  # Importance sampling ratio\n",
        "        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)\n",
        "        return_t = 0.0\n",
        "\n",
        "        # Calculate the return for each state-action pair\n",
        "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            N[(state_t, action_t)] += 1\n",
        "            Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])\n",
        "\n",
        "            if action_t != torch.argmax(Q[state_t]).item():\n",
        "                break\n",
        "            W *= 1.0 / behavior_policy(state_t)[action_t]\n",
        "\n",
        "    # Derive the optimal policy from the Q-function\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "mhDhAMOChY9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hlfFP-whhbhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "T35PZkB_dU8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "\n",
        "# Run Monte Carlo control to find the optimal policy and Q-function\n",
        "optimal_Q, optimal_policy = mc_control_off_policy_incremental(env, gamma, n_episode, random_policy)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\\n\", optimal_policy)\n",
        "\n",
        "# Derive the value function from the optimal Q-function\n",
        "optimal_value = defaultdict(float)\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\\n\", optimal_value)\n",
        "\n",
        "# Plot the optimal value function\n",
        "plot_blackjack_value(optimal_value)"
      ],
      "metadata": {
        "id": "PvtuCxTdikPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Monte Carlo Control with Weighted Importance Sampling"
      ],
      "metadata": {
        "id": "RbeefynykwDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Theory:\n",
        "**Weighted importance sampling** is an advanced method in off-policy Monte Carlo control. It adjusts the updates to the Q-function using the cumulative importance sampling ratios, ensuring that the estimates remain unbiased and efficient.\n",
        "\n",
        "**Steps:**\n",
        "1. **Behavior Policy:** Define a behavior policy for generating episodes.\n",
        "2. **Simulate Episodes:** Generate episodes following the behavior policy.\n",
        "3. **Importance Sampling:** Calculate the cumulative importance sampling ratios to weight the returns appropriately.\n",
        "4. **Update Q-Function:** Update the Q-function using weighted returns to reflect the true value under the target policy.\n",
        "5. **Improve Policy:** Derive the optimal policy based on the updated Q-function.\n"
      ],
      "metadata": {
        "id": "3aSe2YKojwuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "AnkZxOHlh8lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "id": "TvV1W0PNh90q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_random_policy(n_action):\n",
        "    \"\"\"\n",
        "    Generates a random policy with equal probability for each action.\n",
        "\n",
        "    Parameters:\n",
        "    n_action (int): Number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "    function: A policy function that returns action probabilities for a given state.\n",
        "    \"\"\"\n",
        "    probs = torch.ones(n_action) / n_action\n",
        "\n",
        "    def policy_function(state):\n",
        "        return probs\n",
        "\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "ktcp-gZXh_9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = gen_random_policy(env.action_space.n)"
      ],
      "metadata": {
        "id": "66IVcLT6iDxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, behavior_policy):\n",
        "    \"\"\"\n",
        "    Executes an episode following the given behavior policy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    states (list): List of states encountered during the episode.\n",
        "    actions (list): List of actions taken during the episode.\n",
        "    rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        probs = behavior_policy(state)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "10ijEJ9siGes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):\n",
        "    \"\"\"\n",
        "    Finds the optimal policy using off-policy Monte Carlo control with weighted importance sampling.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    gamma (float): Discount factor.\n",
        "    n_episode (int): Number of episodes to run.\n",
        "    behavior_policy (function): The behavior policy function.\n",
        "\n",
        "    Returns:\n",
        "    Q (defaultdict): The optimal Q-function.\n",
        "    policy (dict): The optimal policy.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    N = defaultdict(float)\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        W = 1.0  # Importance sampling ratio\n",
        "        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)\n",
        "        return_t = 0.0\n",
        "\n",
        "        # Calculate the return for each state-action pair\n",
        "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
        "            return_t = gamma * return_t + reward_t\n",
        "            N[(state_t, action_t)] += W\n",
        "            Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])\n",
        "\n",
        "            if action_t != torch.argmax(Q[state_t]).item():\n",
        "                break\n",
        "            W *= 1.0 / behavior_policy(state_t)[action_t]\n",
        "\n",
        "    # Derive the optimal policy from the Q-function\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "ziaSr5ZgiJBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Simulates an episode using a given policy.\n",
        "\n",
        "    Parameters:\n",
        "    env (gym.Env): The Blackjack environment.\n",
        "    policy (dict): The policy to follow.\n",
        "\n",
        "    Returns:\n",
        "    reward (int): The reward received at the end of the episode.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        action = policy[state]\n",
        "        state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "dbVUhVvwiLHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_surface(X, Y, Z, title):\n",
        "    \"\"\"\n",
        "    Plot a 3D surface.\n",
        "\n",
        "    Parameters:\n",
        "    X (array): Meshgrid array for X-axis.\n",
        "    Y (array): Meshgrid array for Y-axis.\n",
        "    Z (array): Values for Z-axis.\n",
        "    title (str): Plot title.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Player Sum')\n",
        "    ax.set_ylabel('Dealer Showing')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.view_init(ax.elev, -120)\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "I_r9tu0xiNG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_value(V):\n",
        "    \"\"\"\n",
        "    Plot the value function for Blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    V (defaultdict): The estimated value function.\n",
        "    \"\"\"\n",
        "    player_sum_range = range(12, 22)\n",
        "    dealer_show_range = range(1, 11)\n",
        "    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])\n",
        "\n",
        "    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))\n",
        "\n",
        "    for i, player in enumerate(player_sum_range):\n",
        "        for j, dealer in enumerate(dealer_show_range):\n",
        "            for k, ace in enumerate([False, True]):\n",
        "                values_to_plot[i, j, k] = V[(player, dealer, ace)]\n",
        "\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 0].numpy(), \"Value Function without Usable Ace\")\n",
        "    plot_surface(X, Y, values_to_plot[:, :, 1].numpy(), \"Value Function with Usable Ace\")"
      ],
      "metadata": {
        "id": "wgRTuKH7iOr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "gamma = 1\n",
        "n_episode = 500000\n",
        "\n",
        "# Run Monte Carlo control to find the optimal policy and Q-function\n",
        "optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\\n\", optimal_policy)\n",
        "\n",
        "# Derive the value function from the optimal Q-function\n",
        "optimal_value = defaultdict(float)\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\\n\", optimal_value)\n",
        "\n",
        "# Plot the optimal value function\n",
        "plot_blackjack_value(optimal_value)"
      ],
      "metadata": {
        "id": "-bbwwImkk3j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate episodes to evaluate the optimal policy\n",
        "n_episode = 100000\n",
        "n_win_optimal = 0\n",
        "n_lose_optimal = 0\n",
        "\n",
        "for _ in range(n_episode):\n",
        "    reward = simulate_episode(env, optimal_policy)\n",
        "    if reward == 1:\n",
        "        n_win_optimal += 1\n",
        "    elif reward == -1:\n",
        "        n_lose_optimal += 1\n",
        "\n",
        "# Print the win/lose probabilities\n",
        "print('Win probability with optimal policy: {:.2f}'.format(n_win_optimal / n_episode))\n",
        "print('Lose probability with optimal policy: {:.2f}'.format(n_lose_optimal / n_episode))"
      ],
      "metadata": {
        "id": "jVtHMs4Xlnwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}