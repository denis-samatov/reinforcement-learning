{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UVdsKd80oSGH",
        "u_aO4MIrRio0",
        "zUpXbW95RqqA",
        "w4P2jUg9RySc",
        "2G_Xch-5pKfC",
        "SWM3agTfSpjS",
        "3J1AtklbSuy1",
        "9eyuAhMASxtZ",
        "F59ZZTnES2Vk",
        "CmZXqf1wv9Le",
        "2ZQv4klzUOUB",
        "AUi-8lW6UR-o",
        "yquaxKTCUU2f",
        "c_W-Ahd6Ubyk",
        "Ze_8IfgFUfFF",
        "L2WVYY0b2bCk",
        "XYPr0alGVOf8",
        "xkrK_a-NVRtw",
        "wxT4gfiHVVd8",
        "0jOfviuPVZEk",
        "03xSOYGTVb3r",
        "97Ll4iXBVgvN",
        "irLKMNUI6L9s",
        "0z5c-GbeWAf9",
        "AAMQw1G0WEPE",
        "vt-KjrcTWHaP",
        "19HnNdQJWOH6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial: Implementing Q-Learning for CliffWalking"
      ],
      "metadata": {
        "id": "UVdsKd80oSGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tutorial: Implementing Q-Learning for CliffWalking\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Q-Learning is a reinforcement learning algorithm used to find the optimal action-selection policy for any given finite Markov decision process (MDP). In the context of the CliffWalking environment, an agent learns to navigate a grid world to reach a goal while avoiding cliffs.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "- **States (S)**: The agent's current position in the grid.\n",
        "- **Actions (A)**: The possible moves the agent can make (up, down, left, right).\n",
        "- **Rewards (R)**: The feedback the agent receives after taking an action (positive for reaching the goal, negative for falling off the cliff).\n",
        "- **Q-Values (Q)**: The expected future rewards for state-action pairs.\n",
        "- **Policy (Ï€)**: A strategy that defines the action an agent should take in a given state.\n",
        "\n",
        "**Q-Learning Algorithm Steps:**\n",
        "\n",
        "1. **Initialize Q-values**: Initialize the Q-values for all state-action pairs to zero.\n",
        "2. **Choose Action**: Select an action using an epsilon-greedy policy to balance exploration and exploitation.\n",
        "3. **Take Action and Observe**: Execute the chosen action, observe the reward and the new state.\n",
        "4. **Update Q-Values**: Update the Q-values using the Bellman equation:\n",
        "\n",
        "   $$\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "   $$\n",
        "\n",
        "5. **Repeat**: Repeat the process for a specified number of episodes or until convergence.\n",
        "\n",
        "This approach uses LaTeX syntax within Markdown to create well-formatted mathematical expressions. To ensure that these expressions render correctly, make sure that the platform you are using supports LaTeX within Markdown, such as Jupyter Notebooks, GitHub, or certain Markdown editors with LaTeX support."
      ],
      "metadata": {
        "id": "aIFmNqPhRhlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting Up the Environment\n",
        "First, we need to set up the `CliffWalking` environment using OpenAI's `gym` library.\n"
      ],
      "metadata": {
        "id": "u_aO4MIrRio0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\", render_mode='human')\n",
        "n_state = env.observation_space.n\n",
        "print(f\"Number of states: {n_state}\")\n",
        "\n",
        "n_action = env.action_space.n\n",
        "print(f\"Number of actions: {n_action}\")\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "new_state, reward, is_done, info = env.step(2)  # Take a step (example action 2)\n",
        "env.render()\n",
        "\n",
        "print(f\"New state: {new_state}, Reward: {reward}, Done: {is_done}\")"
      ],
      "metadata": {
        "id": "4OzbAg_3RlWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Implementing the Q-Learning Algorithm\n",
        "Next, we implement the Q-Learning algorithm. Q-Learning is an off-policy algorithm that updates the Q-values using the Bellman equation."
      ],
      "metadata": {
        "id": "zUpXbW95RqqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, n_state, n_action, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.n_action = n_action\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = defaultdict(lambda: np.zeros(n_action))  # Initialize Q-table with zeros\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(self.n_action))  # Explore: random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit: best action based on current Q-values\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
        "        td_delta = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_delta\n",
        "\n",
        "    def train(self, env, n_episode):\n",
        "        for episode in range(n_episode):\n",
        "            state = env.reset()\n",
        "            is_done = False\n",
        "            total_reward = 0\n",
        "            while not is_done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, is_done, _ = env.step(action)\n",
        "                self.update_q_table(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "            print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "MaH112u1RtAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "n_episode = 500\n",
        "\n",
        "agent = QLearningAgent(n_state, n_action, alpha, gamma, epsilon)\n",
        "agent.train(env, n_episode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "wKlNc_VxRwNo",
        "outputId": "3c70568b-0b4a-46af-dfc3-868265cc74bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'QLearningAgent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc3ea44cbf1e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'QLearningAgent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Visualizing the Results\n",
        "\n",
        "After training, we can visualize the agent's performance by rendering a few episodes.\n"
      ],
      "metadata": {
        "id": "w4P2jUg9RySc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    is_done = False\n",
        "    total_reward = 0\n",
        "    while not is_done:\n",
        "        env.render()\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, is_done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    print(f\"Test Episode: {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "-u1NAKDFR22n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvements and Advanced Techniques\n",
        "1. **Double Q-Learning**: Implement Double Q-Learning to reduce overestimation bias.\n",
        "2. **Prioritized Experience Replay**: Use prioritized experience replay to sample more important transitions.\n",
        "3. **Dueling Q-Networks**: Separate state value and advantage estimation.\n",
        "4. **Deep Q-Networks (DQN)**: Use a neural network to approximate the Q-value function.\n",
        "\n",
        "### Summary\n",
        "In this tutorial, we set up the CliffWalking environment, implemented a basic Q-Learning algorithm, trained the agent, and visualized the results. By following these steps, you can extend the implementation to more advanced reinforcement learning algorithms and improve the agent's performance."
      ],
      "metadata": {
        "id": "3ojKr3aYR47h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Q-Learning for CliffWalking"
      ],
      "metadata": {
        "id": "2G_Xch-5pKfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implementing Q-Learning for CliffWalking\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "In Q-Learning, the agent updates its knowledge about the environment by iteratively improving its Q-values. The CliffWalking environment presents a challenging scenario where the agent must learn to avoid falling off the cliff while reaching the goal efficiently.\n",
        "\n",
        "#### Epsilon-Greedy Policy:\n",
        "\n",
        "To ensure that the agent explores the state space, we use an epsilon-greedy policy:\n",
        "- With probability \\(\\epsilon\\), choose a random action (exploration).\n",
        "- With probability \\(1 - \\epsilon\\), choose the action with the highest Q-value (exploitation).\n",
        "\n",
        "**Q-Learning Algorithm:**\n",
        "\n",
        "1. **Initialize Q-values**: Initialize the Q-values for all state-action pairs to zero.\n",
        "\n",
        "2. **Choose Action**: Select an action using an epsilon-greedy policy:\n",
        "   - Generate a random number between 0 and 1.\n",
        "   - If the number is less than \\(\\epsilon\\), choose a random action.\n",
        "   - Otherwise, choose the action with the highest Q-value for the current state.\n",
        "\n",
        "3. **Take Action and Observe**: Execute the chosen action, observe the reward and the new state.\n",
        "\n",
        "4. **Update Q-Values**: Update the Q-values using the Bellman equation:\n",
        "   $$\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "   $$\n",
        "   \n",
        "   where:\n",
        "   - \\(s\\) is the current state.\n",
        "   - \\(a\\) is the chosen action.\n",
        "   - \\(r\\) is the observed reward.\n",
        "   - \\(s'\\) is the new state after taking action \\(a\\).\n",
        "   - \\(\\alpha\\) is the learning rate.\n",
        "   - \\(\\gamma\\) is the discount factor.\n",
        "   - \\(a'\\) is the action that maximizes the Q-value for the new state \\(s'\\).\n",
        "\n",
        "5. **Repeat**: Repeat steps 2-4 for each episode until the policy converges or for a specified number of episodes."
      ],
      "metadata": {
        "id": "caTivlFBSnbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries and Set Up the Environment\n",
        "\n",
        "Import the necessary libraries and create the `CliffWalking-v0` environment.\n"
      ],
      "metadata": {
        "id": "SWM3agTfSpjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\")"
      ],
      "metadata": {
        "id": "hdtH7QCYSr1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Generate Epsilon-Greedy Policy\n",
        "\n",
        "The `gen_epsilon_greedy_policy` function creates an epsilon-greedy policy that selects a random action with probability `epsilon` and the best action based on the current Q-values with probability `1 - epsilon`."
      ],
      "metadata": {
        "id": "3J1AtklbSuy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
        "    def policy_function(state, Q):\n",
        "        probs = torch.ones(n_action) * epsilon / n_action\n",
        "        best_action = torch.argmax(Q[state]).item()\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "JHsfkBWJSwTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement the Q-Learning Algorithm\n",
        "\n",
        "The `q_learning` function implements the Q-learning algorithm, updating Q-values and creating the optimal policy."
      ],
      "metadata": {
        "id": "9eyuAhMASxtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, gamma, n_episode, alpha):\n",
        "    n_action = env.action_space.n\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "    length_episode = [0] * n_episode\n",
        "    total_reward_episode = [0] * n_episode\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            action = epsilon_greedy_policy(state, Q)\n",
        "            next_state, reward, is_done, info = env.step(action)\n",
        "            td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
        "            Q[state][action] += alpha * td_delta\n",
        "            length_episode[episode] += 1\n",
        "            total_reward_episode[episode] += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "    return Q, policy, length_episode, total_reward_episode"
      ],
      "metadata": {
        "id": "kpr7HAOCS1Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Run Training and Visualize Results\n",
        "\n",
        "Set the hyperparameters and run the training process. After training is complete, visualize the results."
      ],
      "metadata": {
        "id": "F59ZZTnES2Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1\n",
        "n_episode = 500\n",
        "alpha = 0.4\n",
        "epsilon = 0.1\n",
        "\n",
        "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
        "\n",
        "optimal_Q, optimal_policy, length_episode, total_reward_episode = q_learning(env, gamma, n_episode, alpha)\n",
        "print('Optimal Policy:\\n', optimal_policy)\n",
        "\n",
        "plt.plot(length_episode)\n",
        "plt.title('Episode Length Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward Per Episode Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bhEJWbUiS5TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvements and Extensions\n",
        "\n",
        "1. **Model Saving and Loading**: Add functionality to save and load the Q-table and policy.\n",
        "2. **Double Q-Learning**: Implement the Double Q-Learning algorithm to reduce overestimation bias.\n",
        "3. **Experience Replay**: Add an experience replay mechanism for more stable training.\n",
        "4. **Hyperparameter Tuning**: Make hyperparameters configurable through configuration files or command-line arguments."
      ],
      "metadata": {
        "id": "KuFWEeEOS7y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the Taxi Problem using Q-Learning"
      ],
      "metadata": {
        "id": "CmZXqf1wv9Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Solving the Taxi Problem using Q-Learning\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "The Taxi problem is a classic reinforcement learning task where the agent (a taxi) must pick up and drop off passengers at specified locations on a grid. The goal is to minimize the steps taken while avoiding illegal moves.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "- **State Representation**: The state is represented by the taxi's position, passenger's location, and destination.\n",
        "- **Rewards**: Positive rewards for successfully picking up/dropping off passengers, negative rewards for illegal actions or time steps.\n",
        "- **Action Space**: Actions include moving in four directions, picking up, and dropping off passengers.\n",
        "\n",
        "**Q-Learning Application:**\n",
        "\n",
        "1. **Initialize**: Initialize Q-values for all state-action pairs.\n",
        "2. **Policy**: Use an epsilon-greedy policy to select actions.\n",
        "3. **Learning**: Update Q-values using the observed rewards and future state estimations.\n",
        "4. **Convergence**: Continue until the policy converges or a predefined number of episodes are completed."
      ],
      "metadata": {
        "id": "6SHjKRxzUNLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Set Up the Environment\n",
        "\n",
        "First, set up the Taxi environment and check the number of states and actions.\n"
      ],
      "metadata": {
        "id": "2ZQv4klzUOUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "n_state = env.observation_space.n\n",
        "print(f\"Number of states: {n_state}\")\n",
        "\n",
        "n_action = env.action_space.n\n",
        "print(f\"Number of actions: {n_action}\")\n",
        "\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "iLE-Usf3UQuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 2: Generate Epsilon-Greedy Policy\n",
        "\n",
        "Create an epsilon-greedy policy for action selection during the training process.\n"
      ],
      "metadata": {
        "id": "AUi-8lW6UR-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
        "    def policy_function(state, Q):\n",
        "        probs = torch.ones(n_action) * epsilon / n_action\n",
        "        best_action = torch.argmax(Q[state]).item()\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "35v875pwUTlU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement Q-Learning Algorithm\n",
        "\n",
        "Implement the Q-learning algorithm to update Q-values based on interactions with the environment.\n"
      ],
      "metadata": {
        "id": "yquaxKTCUU2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, gamma, n_episode, alpha):\n",
        "    n_action = env.action_space.n\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "    length_episode = [0] * n_episode\n",
        "    total_reward_episode = [0] * n_episode\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            action = epsilon_greedy_policy(state, Q)\n",
        "            next_state, reward, is_done, info = env.step(action)\n",
        "            td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
        "            Q[state][action] += alpha * td_delta\n",
        "            length_episode[episode] += 1\n",
        "            total_reward_episode[episode] += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "    return Q, policy, length_episode, total_reward_episode"
      ],
      "metadata": {
        "id": "GuyauxejUamc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Set Hyperparameters and Train the Model\n",
        "\n",
        "Define the hyperparameters and train the model using the Q-learning algorithm.\n"
      ],
      "metadata": {
        "id": "c_W-Ahd6Ubyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_episode = 1000\n",
        "gamma = 1\n",
        "alpha = 0.4\n",
        "epsilon = 0.1\n",
        "\n",
        "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
        "\n",
        "optimal_Q, optimal_policy, length_episode, total_reward_episode = q_learning(env, gamma, n_episode, alpha)\n",
        "print('Optimal Policy:\\n', optimal_policy)"
      ],
      "metadata": {
        "id": "4ZqzARrfUd_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Visualize Results\n",
        "\n",
        "Plot the results to visualize the performance of the trained agent.\n"
      ],
      "metadata": {
        "id": "Ze_8IfgFUfFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(length_episode)\n",
        "plt.title('Episode Length Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward Per Episode Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y-KPeurvUhoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To solve the Taxi problem using the SARSA (State-Action-Reward-State-Action) algorithm"
      ],
      "metadata": {
        "id": "L2WVYY0b2bCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Solving the Taxi Problem using the SARSA Algorithm\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm. This means that SARSA updates its Q-values based on the action actually taken by the policy, in contrast to Q-Learning, which updates based on the maximum future Q-value. This makes SARSA more sensitive to the policy being followed, as it incorporates the action-selection strategy into its updates.\n",
        "\n",
        "#### SARSA Algorithm Steps:\n",
        "\n",
        "1. **Initialize**: Initialize Q-values for all state-action pairs.\n",
        "2. **Choose Action**: Select an action using the current policy (epsilon-greedy).\n",
        "3. **Take Action and Observe**: Execute the action, observe the reward, next state, and next action.\n",
        "4. **Update Q-Values**: Update Q-values using the SARSA update rule:\n",
        "   $$\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a)  \\right]\n",
        "   $$\n",
        "\n",
        "5. **Repeat**: Repeat the process for each episode until convergence.\n"
      ],
      "metadata": {
        "id": "H_g2EKzqXftH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Set up the environment\n"
      ],
      "metadata": {
        "id": "XYPr0alGVOf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "n_state = env.observation_space.n\n",
        "print(f\"Number of states: {n_state}\")\n",
        "\n",
        "n_action = env.action_space.n\n",
        "print(f\"Number of actions: {n_action}\")\n",
        "\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "REVgZeqdVP17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Implement the epsilon-greedy policy\n"
      ],
      "metadata": {
        "id": "xkrK_a-NVRtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
        "    def policy_function(state, Q):\n",
        "        probs = torch.ones(n_action) * epsilon / n_action\n",
        "        best_action = torch.argmax(Q[state]).item()\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "erpGffieVT2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement the SARSA algorithm"
      ],
      "metadata": {
        "id": "wxT4gfiHVVd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, gamma, n_episode, alpha):\n",
        "    n_action = env.action_space.n\n",
        "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
        "    length_episode = [0] * n_episode\n",
        "    total_reward_episode = [0] * n_episode\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        action = epsilon_greedy_policy(state, Q)\n",
        "        while not is_done:\n",
        "            next_state, reward, is_done, info = env.step(action)\n",
        "            next_action = epsilon_greedy_policy(next_state, Q)\n",
        "            td_delta = reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
        "            Q[state][action] += alpha * td_delta\n",
        "            length_episode[episode] += 1\n",
        "            total_reward_episode[episode] += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}\n",
        "    return Q, policy, length_episode, total_reward_episode"
      ],
      "metadata": {
        "id": "0htoxKSJVYB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train the agent\n"
      ],
      "metadata": {
        "id": "0jOfviuPVZEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1\n",
        "alpha = 0.4\n",
        "epsilon = 0.1\n",
        "\n",
        "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
        "\n",
        "optimal_Q, optimal_policy, length_episode, total_reward_episode = sarsa(env, gamma, n_episode, alpha)\n",
        "print('Optimal Policy:\\n', optimal_policy)"
      ],
      "metadata": {
        "id": "PGqVAlZzVa2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 5: Visualize the results"
      ],
      "metadata": {
        "id": "03xSOYGTVb3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(length_episode)\n",
        "plt.title('Episode Length Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward Per Episode Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDznKD3CVfCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Hyperparameter tuning"
      ],
      "metadata": {
        "id": "97Ll4iXBVgvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_options = [0.4, 0.5, 0.6]\n",
        "epsilon_options = [0.1, 0.03, 0.01]\n",
        "n_episode = 500\n",
        "\n",
        "for alpha in alpha_options:\n",
        "    for epsilon in epsilon_options:\n",
        "        length_episode = [0] * n_episode\n",
        "        total_reward_episode = [0] * n_episode\n",
        "        epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
        "        sarsa(env, gamma, n_episode, alpha)\n",
        "        reward_per_step = [reward/float(step) for reward, step in zip(total_reward_episode, length_episode)]\n",
        "\n",
        "        print('alpha: {}, epsilon: {}'.format(alpha, epsilon))\n",
        "        print('Average reward over {} episodes: {}'.format(n_episode, sum(total_reward_episode) / n_episode))\n",
        "        print('Average length of {} episodes: {}'.format(n_episode, sum(length_episode) / n_episode))\n",
        "        print('Average reward per step over {} episodes: {}\\n'.format(n_episode, sum(reward_per_step) / n_episode))"
      ],
      "metadata": {
        "id": "gHIesXDsViNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Double Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "irLKMNUI6L9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Implementing Double Q-Learning Algorithm\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Double Q-Learning is an extension of Q-Learning designed to reduce overestimation bias by maintaining two separate Q-value estimates. Each update step randomly selects one of the Q-value tables to update, using the other to determine the action to take.\n",
        "\n",
        "#### Double Q-Learning Algorithm Steps:\n",
        "\n",
        "1. **Initialize**: Initialize two Q-tables, \\( Q_1 \\) and \\( Q_2 \\).\n",
        "2. **Choose Action**: Select an action using an epsilon-greedy policy based on the combined Q-values:\n",
        "   $$\n",
        "   Q(s, a) = Q_1(s, a) + Q_2(s, a)\n",
        "   $$\n",
        "3. **Take Action and Observe**: Execute the action, observe the reward and the next state.\n",
        "4. **Update Q-Values**:\n",
        "   - With probability 0.5, update \\( Q_1 \\):\n",
        "     $$\n",
        "     Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha \\left[ r + \\gamma Q_2(s', \\arg\\max_a Q_1(s', a)) - Q_1(s, a) \\right]\n",
        "     $$\n",
        "   - With probability 0.5, update \\( Q_2 \\):\n",
        "     $$\n",
        "     Q_2(s, a) \\leftarrow Q_2(s, a) + \\alpha \\left[ r + \\gamma Q_1(s', \\arg\\max_a Q_2(s', a)) - Q_2(s, a) \\right]\n",
        "     $$\n",
        "5. **Repeat**: Repeat the process for each episode until convergence."
      ],
      "metadata": {
        "id": "So3DTvKxV_uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Set up the environment\n"
      ],
      "metadata": {
        "id": "0z5c-GbeWAf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "n_episode = 3000\n",
        "length_episode = [0] * n_episode\n",
        "total_reward_episode = [0] * n_episode\n",
        "\n",
        "n_state = env.observation_space.n\n",
        "n_action = env.action_space.n\n",
        "\n",
        "print(f\"Number of states: {n_state}\")\n",
        "print(f\"Number of actions: {n_action}\")\n",
        "\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "4IsLMd0wWCcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Implement the epsilon-greedy policy"
      ],
      "metadata": {
        "id": "AAMQw1G0WEPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
        "    def policy_function(state, Q):\n",
        "        probs = torch.ones(n_action) * epsilon / n_action\n",
        "        best_action = torch.argmax(Q[state]).item()\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "IrPjBJmFWGMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement the Double Q-Learning Algorithm\n"
      ],
      "metadata": {
        "id": "vt-KjrcTWHaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def double_q_learning(env, gamma, n_episode, alpha):\n",
        "    \"\"\"\n",
        "    Builds the optimal policy using the Double Q-learning algorithm with\n",
        "    a decoupled strategy.\n",
        "    @param env: OpenAI Gym environment name\n",
        "    @param gamma: discount factor\n",
        "    @param n_episode: number of episodes\n",
        "    @param alpha: learning rate\n",
        "    @return: optimal Q-function and policy\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n\n",
        "    n_state = env.observation_space.n\n",
        "    Q1 = torch.zeros(n_state, n_action)\n",
        "    Q2 = torch.zeros(n_state, n_action)\n",
        "    for episode in range(n_episode):\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            action = epsilon_greedy_policy(state, Q1 + Q2)\n",
        "            next_state, reward, is_done, info = env.step(action)\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                best_next_action = torch.argmax(Q1[next_state])\n",
        "                td_delta = reward + gamma * Q2[next_state][best_next_action] - Q1[state][action]\n",
        "                Q1[state][action] += alpha * td_delta\n",
        "            else:\n",
        "                best_next_action = torch.argmax(Q2[next_state])\n",
        "                td_delta = reward + gamma * Q1[next_state][best_next_action] - Q2[state][action]\n",
        "                Q2[state][action] += alpha * td_delta\n",
        "            length_episode[episode] += 1\n",
        "            total_reward_episode[episode] += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = next_state\n",
        "    policy = {}\n",
        "    Q = Q1 + Q2\n",
        "    for state in range(n_state):\n",
        "        policy[state] = torch.argmax(Q[state]).item()\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "itpgdbReWLel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1\n",
        "alpha = 0.4\n",
        "epsilon = 0.1\n",
        "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
        "\n",
        "optimal_Q, optimal_policy = double_q_learning(env, gamma, n_episode, alpha)"
      ],
      "metadata": {
        "id": "jvEb0sf7WM2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Visualize the Results"
      ],
      "metadata": {
        "id": "19HnNdQJWOH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(length_episode)\n",
        "plt.title('Episode Length Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward Per Episode Over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c7PCYU7IWPrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "These reinforcement learning algorithms help agents learn optimal policies by iteratively updating their Q-values based on the rewards received from the environment. Q-Learning, SARSA, and Double Q-Learning each have their own strategies for balancing exploration and exploitation, updating Q-values, and reducing bias, making them suitable for different types of problems."
      ],
      "metadata": {
        "id": "5iiomAJ5WTRz"
      }
    }
  ]
}