{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_1BxwLIIlvlY",
        "ETIZr-TYc2w_",
        "YzEZc2LNj7xn",
        "PTzKjFHermsB",
        "aReOqfTp7u9z",
        "0eQvYT4mBnqP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial: Implementing and Improving Deep Q-Networks (DQN)"
      ],
      "metadata": {
        "id": "_1BxwLIIlvlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Implementation of Deep Q-Networks (DQN)\n",
        "Description:\n",
        "Deep Q-Networks (DQN) use neural networks to approximate the Q-value function in reinforcement learning, which predicts the total reward an agent can expect to receive, starting from a given state and taking a specific action. The DQN algorithm uses experience replay and target networks to stabilize training."
      ],
      "metadata": {
        "id": "ETIZr-TYc2w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OY-wZlS8g_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.001):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_action)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Updates DQN weights given a training sample.\n",
        "        @param s: state\n",
        "        @param y: target value\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values for all actions in a given state using the trained model.\n",
        "        @param s: input state\n",
        "        @return: Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))"
      ],
      "metadata": {
        "id": "92XTCrr2hC0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "0CNufzwkhD0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "3D1FGqh7hFLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, estimator, n_episode, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
        "    \"\"\"\n",
        "    Deep Q-learning using DQN.\n",
        "    @param env: Gym environment\n",
        "    @param estimator: DQN estimator object\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: discount factor\n",
        "    @param epsilon: epsilon-greedy strategy parameter\n",
        "    @param epsilon_decay: epsilon decay rate\n",
        "    \"\"\"\n",
        "    total_reward_episode = [0] * n_episode\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, env.action_space.n)\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            # Modify reward based on the position of the car\n",
        "            modified_reward = reward + (next_state[0] + 0.5)\n",
        "            if next_state[0] >= 0.5:\n",
        "                modified_reward += 100\n",
        "            elif next_state[0] >= 0.25:\n",
        "                modified_reward += 20\n",
        "            elif next_state[0] >= 0.1:\n",
        "                modified_reward += 10\n",
        "            elif next_state[0] >= 0:\n",
        "                modified_reward += 5\n",
        "\n",
        "            q_values = estimator.predict(state).tolist()\n",
        "\n",
        "            if is_done:\n",
        "                q_values[action] = modified_reward\n",
        "            else:\n",
        "                q_values_next = estimator.predict(next_state)\n",
        "                q_values[action] = modified_reward + gamma * torch.max(q_values_next).item()\n",
        "\n",
        "            estimator.update(state, q_values)\n",
        "            state = next_state\n",
        "\n",
        "        print(f'Episode: {episode}, Total reward: {total_reward_episode[episode]}, Epsilon: {epsilon}')\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    return total_reward_episode"
      ],
      "metadata": {
        "id": "sWKccvUQhJBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden = 50\n",
        "lr = 0.001\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "n_episode = 1000\n",
        "total_reward_episode = q_learning(env, dqn, n_episode, gamma=0.99, epsilon=0.3)"
      ],
      "metadata": {
        "id": "g1xQYBi6g8N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward per Episode over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZcL4OuL1hLrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Improving DQN with Experience Replay\n",
        "\n",
        "Description:\n",
        "Experience replay improves the stability and efficiency of DQN by storing the agent's experiences in a replay buffer and sampling random batches from it for training. This helps in breaking the correlation between consecutive experiences and provides more diverse training data."
      ],
      "metadata": {
        "id": "YzEZc2LNj7xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "K72djRPBhr8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "-fpRsMWbhs0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.001):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_action)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Updates the DQN weights given a training sample.\n",
        "        @param s: state\n",
        "        @param y: target value\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values for all actions in a given state using the trained model.\n",
        "        @param s: input state\n",
        "        @return: Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))\n",
        "\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        \"\"\"\n",
        "        Experience replay.\n",
        "        @param memory: experience replay buffer\n",
        "        @param replay_size: number of samples to use for model update\n",
        "        @param gamma: discount factor\n",
        "        \"\"\"\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            states = []\n",
        "            td_targets = []\n",
        "            for state, action, next_state, reward, is_done in replay_data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if is_done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.predict(next_state)\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "                td_targets.append(q_values)\n",
        "\n",
        "            self.update(states, td_targets)"
      ],
      "metadata": {
        "id": "ocMRQmX8hv00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "VtUwvR98h1V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
        "    \"\"\"\n",
        "    Deep Q-learning with DQN and experience replay.\n",
        "    @param env: Gym environment\n",
        "    @param estimator: DQN estimator object\n",
        "    @param replay_size: number of samples to use for model update\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: discount factor\n",
        "    @param epsilon: epsilon-greedy strategy parameter\n",
        "    @param epsilon_decay: epsilon decay rate\n",
        "    \"\"\"\n",
        "    total_reward_episode = [0] * n_episode\n",
        "    memory = deque(maxlen=10000)\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, env.action_space.n)\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "        while not is_done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            # Modify reward based on the position of the car\n",
        "            modified_reward = reward + (next_state[0] + 0.5)\n",
        "            if next_state[0] >= 0.5:\n",
        "                modified_reward += 100\n",
        "            elif next_state[0] >= 0.25:\n",
        "                modified_reward += 20\n",
        "            elif next_state[0] >= 0.1:\n",
        "                modified_reward += 10\n",
        "            elif next_state[0] >= 0:\n",
        "                modified_reward += 5\n",
        "\n",
        "            memory.append((state, action, next_state, modified_reward, is_done))\n",
        "            if is_done:\n",
        "                break\n",
        "            estimator.replay(memory, replay_size, gamma)\n",
        "            state = next_state\n",
        "\n",
        "        print(f'Episode: {episode}, Total reward: {total_reward_episode[episode]}, Epsilon: {epsilon}')\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    return total_reward_episode"
      ],
      "metadata": {
        "id": "mHRs3OCKh4pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden = 50\n",
        "lr = 0.001\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "n_episode = 600\n",
        "replay_size = 20\n",
        "total_reward_episode = q_learning(env, dqn, n_episode, replay_size, gamma=0.9, epsilon=0.3)"
      ],
      "metadata": {
        "id": "NZ12r5qdhpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward per Episode over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JKLV6MWKh91b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Implementing Double DQN\n",
        "\n",
        "Description:\n",
        "Double DQN addresses the overestimation bias in Q-learning by decoupling the selection of actions and the evaluation of Q-values. It uses two networks: a primary network for selecting actions and a target network for evaluating the Q-values of those actions."
      ],
      "metadata": {
        "id": "PTzKjFHermsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ct6bKdi3iwfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "8sOmAP0Dixgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.01):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_action)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "        self.model_target = copy.deepcopy(self.model)\n",
        "\n",
        "    def target_predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values using the target network.\n",
        "        @param s: input state\n",
        "        @return: target Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model_target(torch.Tensor(s))\n",
        "\n",
        "    def copy_target(self):\n",
        "        \"\"\"\n",
        "        Copies the parameters from the main network to the target network.\n",
        "        \"\"\"\n",
        "        self.model_target.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        \"\"\"\n",
        "        Experience replay with the target network.\n",
        "        @param memory: experience replay buffer\n",
        "        @param replay_size: number of samples to use for model update\n",
        "        @param gamma: discount factor\n",
        "        \"\"\"\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            states = []\n",
        "            td_targets = []\n",
        "            for state, action, next_state, reward, is_done in replay_data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if is_done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.target_predict(next_state).detach()\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "\n",
        "                td_targets.append(q_values)\n",
        "\n",
        "            self.update(states, td_targets)\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Updates DQN weights given a training sample.\n",
        "        @param s: state\n",
        "        @param y: target value\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values for all actions in a given state using the trained model.\n",
        "        @param s: input state\n",
        "        @return: Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))"
      ],
      "metadata": {
        "id": "Rc_s70oji1GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Generates an epsilon-greedy policy.\n",
        "    @param estimator: DQN estimator\n",
        "    @param epsilon: exploration rate\n",
        "    @param n_action: number of actions\n",
        "    @return: epsilon-greedy policy function\n",
        "    \"\"\"\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "rcednkKyi3Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, estimator, n_episode, replay_size, target_update=10, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
        "    \"\"\"\n",
        "    Double DQN with experience replay.\n",
        "    @param env: Gym environment\n",
        "    @param estimator: DQN estimator\n",
        "    @param replay_size: number of samples for model update\n",
        "    @param target_update: update target network every `target_update` episodes\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: discount factor\n",
        "    @param epsilon: epsilon-greedy strategy parameter\n",
        "    @param epsilon_decay: epsilon decay rate\n",
        "    \"\"\"\n",
        "    total_reward_episode = [0] * n_episode\n",
        "    memory = deque(maxlen=10000)\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        if episode % target_update == 0:\n",
        "            estimator.copy_target()\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, env.action_space.n)\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "\n",
        "        while not is_done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            # Modify reward based on the position of the car\n",
        "            modified_reward = reward + (next_state[0] + 0.5)\n",
        "            if next_state[0] >= 0.5:\n",
        "                modified_reward += 100\n",
        "            elif next_state[0] >= 0.25:\n",
        "                modified_reward += 20\n",
        "            elif next_state[0] >= 0.1:\n",
        "                modified_reward += 10\n",
        "            elif next_state[0] >= 0:\n",
        "                modified_reward += 5\n",
        "\n",
        "            memory.append((state, action, next_state, modified_reward, is_done))\n",
        "\n",
        "            if is_done:\n",
        "                break\n",
        "\n",
        "            estimator.replay(memory, replay_size, gamma)\n",
        "            state = next_state\n",
        "        print(f'Episode: {episode}, Total reward: {total_reward_episode[episode]}, Epsilon: {epsilon}')\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    return total_reward_episode"
      ],
      "metadata": {
        "id": "QSma1AH3i5p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden = 50\n",
        "lr = 0.01\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "n_episode = 600\n",
        "replay_size = 20\n",
        "target_update = 10\n",
        "\n",
        "total_reward_episode = q_learning(env, dqn, n_episode, replay_size, target_update=target_update, gamma=0.9, epsilon=0.3)"
      ],
      "metadata": {
        "id": "vIt1LpIii7gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward per Episode over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "me-j980xiusb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Hyperparameter Tuning for Double DQN in CartPole Environment\n",
        "\n",
        "Description:\n",
        "Hyperparameter tuning involves experimenting with different values for the hyperparameters of a model to find the combination that results in the best performance. For Double DQN, important hyperparameters include learning rate, hidden layer size, replay buffer size, and target network update frequency."
      ],
      "metadata": {
        "id": "aReOqfTp7u9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "a15D1eGhjs2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"CartPole-v0\")"
      ],
      "metadata": {
        "id": "-5XYoWjljui-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.01):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_action)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "        self.model_target = copy.deepcopy(self.model)\n",
        "\n",
        "    def target_predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values using the target network.\n",
        "        @param s: input state\n",
        "        @return: target Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model_target(torch.Tensor(s))\n",
        "\n",
        "    def copy_target(self):\n",
        "        \"\"\"\n",
        "        Copies the parameters from the main network to the target network.\n",
        "        \"\"\"\n",
        "        self.model_target.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        \"\"\"\n",
        "        Experience replay with the target network.\n",
        "        @param memory: experience replay buffer\n",
        "        @param replay_size: number of samples to use for model update\n",
        "        @param gamma: discount factor\n",
        "        \"\"\"\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            states = []\n",
        "            td_targets = []\n",
        "            for state, action, next_state, reward, is_done in replay_data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if is_done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.target_predict(next_state).detach()\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "\n",
        "                td_targets.append(q_values)\n",
        "\n",
        "            self.update(states, td_targets)\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Updates DQN weights given a training sample.\n",
        "        @param s: state\n",
        "        @param y: target value\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values for all actions in a given state using the trained model.\n",
        "        @param s: input state\n",
        "        @return: Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))"
      ],
      "metadata": {
        "id": "9HWdbW7GjzWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Generates an epsilon-greedy policy.\n",
        "    @param estimator: DQN estimator\n",
        "    @param epsilon: exploration rate\n",
        "    @param n_action: number of actions\n",
        "    @return: epsilon-greedy policy function\n",
        "    \"\"\"\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "OU9PYEd5j21d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, estimator, n_episode, replay_size, target_update=10, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
        "    \"\"\"\n",
        "    Double DQN with experience replay.\n",
        "    @param env: Gym environment\n",
        "    @param estimator: DQN estimator\n",
        "    @param replay_size: number of samples for model update\n",
        "    @param target_update: update target network every `target_update` episodes\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: discount factor\n",
        "    @param epsilon: epsilon-greedy strategy parameter\n",
        "    @param epsilon_decay: epsilon decay rate\n",
        "    \"\"\"\n",
        "    total_reward_episode = [0] * n_episode\n",
        "    memory = deque(maxlen=10000)\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        if episode % target_update == 0:\n",
        "            estimator.copy_target()\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, env.action_space.n)\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "\n",
        "        while not is_done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            memory.append((state, action, next_state, reward, is_done))\n",
        "\n",
        "            if is_done:\n",
        "                break\n",
        "\n",
        "            estimator.replay(memory, replay_size, gamma)\n",
        "            state = next_state\n",
        "\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    return total_reward_episode"
      ],
      "metadata": {
        "id": "164nhQo1j6_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter options\n",
        "n_hidden_options = [30, 40]\n",
        "lr_options = [0.001, 0.003]\n",
        "replay_size_options = [20, 25]\n",
        "target_update_options = [30, 35]\n",
        "\n",
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_episode = 600\n",
        "last_episode = 200\n",
        "\n",
        "# Grid search for hyperparameter tuning\n",
        "results = []\n",
        "\n",
        "for n_hidden in n_hidden_options:\n",
        "    for lr in lr_options:\n",
        "        for replay_size in replay_size_options:\n",
        "            for target_update in target_update_options:\n",
        "                env.seed(1)\n",
        "                random.seed(1)\n",
        "                torch.manual_seed(1)\n",
        "                dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "                total_reward_episode = q_learning(env, dqn, n_episode, replay_size, target_update, gamma=0.9, epsilon=1)\n",
        "                average_reward = sum(total_reward_episode[-last_episode:]) / last_episode\n",
        "                results.append((n_hidden, lr, replay_size, target_update, average_reward))\n",
        "                print(f\"n_hidden: {n_hidden}, lr: {lr}, replay_size: {replay_size}, target_update: {target_update}, average_reward: {average_reward}\")\n",
        "\n",
        "# Displaying the best result\n",
        "best_params = max(results, key=lambda x: x[4])\n",
        "print(f\"Best parameters: n_hidden: {best_params[0]}, lr: {best_params[1]}, replay_size: {best_params[2]}, target_update: {best_params[3]}, average_reward: {best_params[4]}\")\n",
        "\n",
        "# Plotting the best result\n",
        "env.seed(1)\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "dqn = DQN(n_state, n_action, best_params[0], best_params[1])\n",
        "total_reward_episode = q_learning(env, dqn, n_episode, best_params[2], target_update=best_params[3], gamma=0.9, epsilon=1)"
      ],
      "metadata": {
        "id": "PsEPCYNYkBMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward per Episode over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EktH9WHB7MSh",
        "outputId": "9a490df3-bd1a-41e2-dca1-e11cc7a143dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Implementing Dueling DQN\n",
        "\n",
        "Description:\n",
        "Dueling DQN introduces a new neural network architecture that separately estimates the state value function and the advantage function for each action. This allows the agent to learn which states are valuable without needing to know the value of each action in those states."
      ],
      "metadata": {
        "id": "0eQvYT4mBnqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ybNoJwzxkZFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.envs.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "zA_xtuZ-kagg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingModel(nn.Module):\n",
        "    def __init__(self, n_input, n_output, n_hidden):\n",
        "        super(DuelingModel, self).__init__()\n",
        "        self.adv1 = nn.Linear(n_input, n_hidden)\n",
        "        self.adv2 = nn.Linear(n_hidden, n_output)\n",
        "        self.val1 = nn.Linear(n_input, n_hidden)\n",
        "        self.val2 = nn.Linear(n_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        adv = nn.functional.relu(self.adv1(x))\n",
        "        adv = self.adv2(adv)\n",
        "        val = nn.functional.relu(self.val1(x))\n",
        "        val = self.val2(val)\n",
        "        return val + adv - adv.mean()"
      ],
      "metadata": {
        "id": "cIic95eTkeP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.001):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = DuelingModel(n_state, n_action, n_hidden)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Updates the DQN weights given a training sample.\n",
        "        @param s: state\n",
        "        @param y: target value\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts Q-values for all actions in a given state using the trained model.\n",
        "        @param s: input state\n",
        "        @return: Q-values for all actions\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))\n",
        "\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        \"\"\"\n",
        "        Experience replay.\n",
        "        @param memory: experience replay buffer\n",
        "        @param replay_size: number of samples to use for model update\n",
        "        @param gamma: discount factor\n",
        "        \"\"\"\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            states = []\n",
        "            td_targets = []\n",
        "            for state, action, next_state, reward, is_done in replay_data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if is_done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.predict(next_state)\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "                td_targets.append(q_values)\n",
        "\n",
        "            self.update(states, td_targets)"
      ],
      "metadata": {
        "id": "75ZcTsKakko5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Generates an epsilon-greedy policy.\n",
        "    @param estimator: DQN estimator\n",
        "    @param epsilon: exploration rate\n",
        "    @param n_action: number of actions\n",
        "    @return: epsilon-greedy policy function\n",
        "    \"\"\"\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ],
      "metadata": {
        "id": "kHe1DsDHkmaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
        "    \"\"\"\n",
        "    Deep Q-learning with DQN and experience replay.\n",
        "    @param env: Gym environment\n",
        "    @param estimator: DQN estimator object\n",
        "    @param replay_size: number of samples for model update\n",
        "    @param n_episode: number of episodes\n",
        "    @param gamma: discount factor\n",
        "    @param epsilon: epsilon-greedy strategy parameter\n",
        "    @param epsilon_decay: epsilon decay rate\n",
        "    \"\"\"\n",
        "    total_reward_episode = [0] * n_episode\n",
        "    memory = deque(maxlen=10000)\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, env.action_space.n)\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "\n",
        "        while not is_done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            # Modify reward based on the position of the car\n",
        "            modified_reward = reward + (next_state[0] + 0.5)\n",
        "            if next_state[0] >= 0.5:\n",
        "                modified_reward += 100\n",
        "            elif next_state[0] >= 0.25:\n",
        "                modified_reward += 20\n",
        "            elif next_state[0] >= 0.1:\n",
        "                modified_reward += 10\n",
        "            elif next_state[0] >= 0:\n",
        "                modified_reward += 5\n",
        "\n",
        "            memory.append((state, action, next_state, modified_reward, is_done))\n",
        "\n",
        "            if is_done:\n",
        "                break\n",
        "\n",
        "            estimator.replay(memory, replay_size, gamma)\n",
        "            state = next_state\n",
        "\n",
        "        print(f'Episode: {episode}, Total reward: {total_reward_episode[episode]}, Epsilon: {epsilon}')\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "\n",
        "    return total_reward_episode"
      ],
      "metadata": {
        "id": "66riTcD_kW6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden = 50\n",
        "lr = 0.001\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "n_episode = 600\n",
        "replay_size = 20\n",
        "\n",
        "total_reward_episode = q_learning(env, dqn, n_episode, replay_size, gamma=0.9, epsilon=0.3)"
      ],
      "metadata": {
        "id": "qIqH1jRAkt21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_reward_episode)\n",
        "plt.title('Total Reward per Episode over Time')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsMtrw9xCLuw",
        "outputId": "7d7bb1f0-dbe8-4ba4-982a-e2d16a057fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "Deep Q-Networks (DQN): Use neural networks to approximate Q-values, enabling agents to learn optimal policies from high-dimensional state spaces.\n",
        "Experience Replay: Stabilizes DQN training by storing experiences and sampling random batches for training, reducing correlations between consecutive updates.\n",
        "Double DQN: Reduces overestimation bias by decoupling action selection and evaluation, using separate primary and target networks.\n",
        "Hyperparameter Tuning for Double DQN: Involves systematically experimenting with different hyperparameters to optimize performance in a specific environment (e.g., CartPole).\n",
        "Dueling DQN: Improves Q-value estimation by separating state value and advantage functions, allowing better generalization and learning efficiency."
      ],
      "metadata": {
        "id": "LYC4gV4WnItt"
      }
    }
  ]
}